{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Classification - Tata Steel Machine Failure Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focused on developing a machine learning pipeline to predict machine failures using sensor and operational data. The process involved data preprocessing, feature engineering, and model training with an emphasis on maximizing recall, since missing a failure can be costly in industrial settings. After encoding categorical variables and standardizing numerical features, the data was split into training and validation sets. Two powerful gradient boosting algorithms â€” XGBoost and LightGBM â€” were implemented and tuned using RandomizedSearchCV to efficiently explore hyperparameter spaces while optimizing for recall. Computational limitations on Windows were addressed by restricting parallel processing (n_jobs=1) and reducing the number of parameter combinations, ensuring stable execution. Both models were evaluated using ROC-AUC and precision-recall curves, with threshold tuning applied to maximize recall. The tuned models demonstrated strong predictive power, high recall, and effective generalization, making them suitable for real-world predictive maintenance applications where detecting potential failures is crucial to prevent downtime and optimize operational efficiency."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting machine failures using sensor data."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "\n",
        "import os, math\n",
        "import pandas as pd, numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = pd.read_csv(\"/content/train.csv\")\n",
        "test_path = pd.read_csv(\"/content/test.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path.columns"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path.duplicated().sum()\n",
        "\n",
        "test_path.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path.isna().sum()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains sensor readings and operational data from industrial machines, aimed at predicting machine failures. Each record represents a machineâ€™s operating state, including parameters such as air temperature, process temperature, rotational speed, torque, and tool wear. A categorical feature indicates machine type, while the target variable (machine_failure) shows whether a failure occurred. Additional columns like TWF, HDF, PWF, OSF, and RNF specify failure causes. The dataset is structured, numerical, and categorical in nature, often exhibiting class imbalance since machine failures are rare compared to normal operations, making it suitable for predictive maintenance and failure detection modeling."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_path.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path.describe()"
      ],
      "metadata": {
        "id": "V0BnJKS3huOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset includes several key variables. Air temperature and process temperature measure environmental and operational heat conditions, respectively. Rotational speed (rpm) indicates the machineâ€™s spinning rate, while torque (Nm) represents the applied force during operation. Tool wear (min) measures tool usage over time, reflecting wear and tear. Type is a categorical feature denoting machine category (e.g., L, M, H). Machine failure is the target variable (1 = failure, 0 = normal). Additional binary indicators â€” TWF (Tool Wear Failure), HDF (Heat Dissipation Failure), PWF (Power Failure), OSF (Overstrain Failure), and RNF (Random Failure) â€” identify specific failure causes."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column in train_path.columns:\n",
        "    unique_values = train_path[column].unique()\n",
        "    print(f\"ðŸ”¹ {column}: {len(unique_values)} unique values\")\n",
        "    if len(unique_values) < 20:  # Display all values if they are few\n",
        "        print(f\"   Values: {unique_values}\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean column names and preview\n",
        "def clean_cols(cols):\n",
        "    out = []\n",
        "    for c in cols:\n",
        "        nc = c.strip().lower().replace(' ', '_').replace('[','').replace(']','').replace('/','_').replace('-','_')\n",
        "        out.append(nc)\n",
        "    return out\n",
        "\n",
        "train_path.columns = clean_cols(train_path.columns)\n",
        "test_path.columns = clean_cols(test_path.columns)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path.info()"
      ],
      "metadata": {
        "id": "Vd5zIfomicV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'machine_failure' in train_path.columns:\n",
        "    print('\\nMachine failure counts:'); display(train_path['machine_failure'].value_counts())"
      ],
      "metadata": {
        "id": "LdUZMDCRicdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA stats for numeric columns\n",
        "num_cols = train_path.select_dtypes(include=[np.number]).columns.tolist()\n",
        "num_cols = [c for c in num_cols if c not in ['id']]\n",
        "stats_df = pd.DataFrame(index=num_cols, columns=['count','mean','median','mode','std_dev','variance','min','max','range','skewness','kurtosis'])\n",
        "for c in num_cols:\n",
        "    col = train_path[c].dropna()\n",
        "    mode_val = col.mode().iloc[0] if not col.mode().empty else np.nan\n",
        "    stats_df.loc[c,'count'] = col.count()\n",
        "    stats_df.loc[c,'mean'] = col.mean()\n",
        "    stats_df.loc[c,'median'] = col.median()\n",
        "    stats_df.loc[c,'mode'] = mode_val\n",
        "    stats_df.loc[c,'std_dev'] = col.std()\n",
        "    stats_df.loc[c,'variance'] = col.var()\n",
        "    stats_df.loc[c,'min'] = col.min()\n",
        "    stats_df.loc[c,'max'] = col.max()\n",
        "    stats_df.loc[c,'range'] = col.max() - col.min()\n",
        "    stats_df.loc[c,'skewness'] = col.skew()\n",
        "    stats_df.loc[c,'kurtosis'] = col.kurt()\n",
        "stats_df = stats_df.apply(pd.to_numeric, errors='ignore')\n",
        "display(stats_df.sort_index())"
      ],
      "metadata": {
        "id": "uN5HO0QaicjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, several data manipulations and exploratory analyses were performed to understand and prepare the dataset for machine failure prediction. The initial steps included loading and inspecting the data, checking for missing values, and identifying data types for numerical and categorical features. Then, we performed statistical analysis â€” calculating mean, median, mode, standard deviation, range, variance, skewness, and kurtosis â€” to understand data distribution and detect skewed or outlier-prone variables. Categorical features like Type were label-encoded for modeling. Visual analyses such as histograms, boxplots, and scatterplots revealed that higher tool wear and torque are strongly associated with machine failures. Correlation heatmaps showed that rotational speed and torque have a notable negative correlation. The target variable (machine_failure) was highly imbalanced, indicating fewer failure cases compared to normal operations, requiring attention during model training. Overall, the manipulations improved data quality, revealed operational patterns, and provided insights into factors influencing machine failures."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Target distribution ---\n",
        "if 'machine_failure' in train_path.columns:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    train_path['machine_failure'].value_counts().plot(kind='bar', color='teal')\n",
        "    plt.title('Machine Failure Distribution')\n",
        "    plt.xlabel('Failure')\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart was chosen because the target variable machine_failure is categorical (binary: 0 or 1). A bar chart effectively visualizes the frequency of each class, allowing us to clearly see how many instances represent machine failures versus normal operations. It provides an immediate visual understanding of class imbalance, which is crucial for designing an appropriate machine learning model."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that machine failures (1) occur far less frequently than non-failures (0). This indicates a significant class imbalance, where most machines operate normally, and only a small portion experience failures. Such imbalance can bias models toward predicting â€œno failure,â€ reducing recall for actual failure cases."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights can drive a positive business impact. Recognizing class imbalance helps us apply techniques like resampling (SMOTE/undersampling) or adjusting model thresholds, ensuring better detection of rare failures. Accurately predicting failures enables preventive maintenance, reducing downtime, repair costs, and production losses.\n",
        "However, if not handled properly, the imbalance could lead to negative outcomes, such as a model that rarely predicts failures, resulting in missed preventive actions and increased operational risks. Therefore, addressing this imbalance is critical for both predictive accuracy and business efficiency."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Type distribution ---\n",
        "if 'type' in train_path.columns:\n",
        "        plt.figure(figsize=(6,4))\n",
        "        train_path['type'].value_counts().plot(kind='bar', color='coral')\n",
        "        plt.title('Product Type Distribution')\n",
        "        plt.xlabel('Type')\n",
        "        plt.ylabel('Count')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart was selected because the variable type is categorical, representing different product categories (L, M, H). Bar charts are ideal for showing how many instances fall into each category, making it easy to compare the relative frequency or dominance of each product type visually and intuitively."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that the distribution of product types is not uniform â€” one or two types (e.g., M or L) dominate the dataset, while others are less represented. This suggests that certain product categories are produced or monitored more frequently than others, possibly due to higher demand or operational significance."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights can have a positive business impact. Understanding which product types dominate helps in resource allocation, inventory management, and failure pattern analysis. For example, if a specific type shows a higher failure rate, maintenance strategies can be tailored accordingly.\n",
        "However, if the dataset is heavily imbalanced by product type and this imbalance is ignored during model training, it may lead to biased predictions, where the model performs poorly on underrepresented types. This could result in negative growth, as failures in those less common types might go undetected, causing unexpected breakdowns and losses."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Histograms and Boxplots for numeric columns ---\n",
        "for c in num_cols:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    sns.histplot(train_path[c].dropna(), bins=50, ax=axes[0], kde=True, color='steelblue')\n",
        "    axes[0].set_title(f'Histogram of {c}')\n",
        "\n",
        "    sns.boxplot(x=train_path[c].dropna(), ax=axes[1], color='lightgreen')\n",
        "    axes[1].set_title(f'Boxplot of {c}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose histograms and boxplots for numerical columns because they complement each other in revealing different aspects of data distribution. Histograms show the frequency distribution and skewness of the data, while boxplots highlight outliers, spread, and median values. Together, they provide a comprehensive understanding of how each numeric variable behaves, which is crucial for identifying patterns and anomalies in sensor readings."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histograms reveal that most numerical features, such as air temperature, process temperature, and rotational speed, are not normally distributed â€” some may be right-skewed or left-skewed. The boxplots show potential outliers, which could correspond to extreme sensor readings indicating abnormal machine behavior or pre-failure conditions. This suggests that data cleaning and scaling might be necessary before modeling."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights can positively impact the business by improving predictive maintenance. Detecting abnormal values early can help prevent costly machine failures and downtime. Understanding feature distributions also enables more robust model training, leading to more reliable predictions.\n",
        "However, if the outliers are not handled properly or are incorrectly removed, it could cause negative growth, as critical early-warning signs of failures might be lost. Hence, itâ€™s important to treat them carefully, preserving valuable information related to fault detection."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Scatter: rotational speed vs torque ---\n",
        "if 'rotational_speed_rpm' in train_path.columns and 'torque_nm' in train_path.columns:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.scatter(train_path['rotational_speed_rpm'], train_path['torque_nm'], s=6, alpha=0.4, color='purple')\n",
        "    plt.title('Rotational Speed vs Torque')\n",
        "    plt.xlabel('Rotational Speed [rpm]')\n",
        "    plt.ylabel('Torque [Nm]')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a scatter plot because it effectively visualizes the relationship between two continuous variables â€” rotational speed and torque. This type of chart helps identify whether there is a correlation or pattern between them, such as a linear, inverse, or non-linear relationship. In the context of machine performance, scatter plots are particularly useful for understanding how changes in one operational variable (speed) affect another (torque)."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot shows an inverse relationship between rotational speed and torque â€” as rotational speed increases, torque tends to decrease. This aligns with expected mechanical behavior where higher speed often comes with lower torque to maintain energy balance. The plot may also reveal clusters or anomalies, suggesting potential operational modes or early indicators of mechanical strain."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights can positively impact the business by improving machine efficiency monitoring and failure prediction. Understanding the normal operating relationship between torque and speed allows engineers to detect deviations that may indicate wear or malfunction, thus enabling preventive maintenance and reducing downtime.\n",
        "However, if the relationship is misinterpreted or oversimplified (e.g., ignoring outliers that represent warning signals), it could lead to negative growth by missing critical failure indicators, resulting in unexpected breakdowns or inefficiencies."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Failure rate by tool wear bins ---\n",
        "if 'tool_wear_min' in train_path.columns and 'machine_failure' in train_path.columns:\n",
        "    train_path['tool_bin'] = pd.qcut(train_path['tool_wear_min'], q=6, duplicates='drop')\n",
        "    grp = train_path.groupby('tool_bin')['machine_failure'].mean()\n",
        "    plt.figure(figsize=(8,4))\n",
        "    grp.plot(kind='bar', color='tomato')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.title('Failure Rate by Tool Wear Bins')\n",
        "    plt.xlabel('Tool Wear Bins')\n",
        "    plt.ylabel('Failure Rate')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart was chosen because it clearly shows how the failure rate varies across different ranges (bins) of tool wear. By grouping tool wear into quantile-based bins, we can easily compare failure proportions between lower and higher wear levels. This type of visualization helps identify thresholds where the likelihood of failure significantly increases, making it intuitive and actionable for maintenance teams."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that machine failure rates increase with higher tool wear. As the tool wear progresses, the probability of machine failure tends to rise sharply after certain thresholds, indicating that worn tools contribute significantly to operational breakdowns. This highlights tool degradation as a key predictive factor in machine reliability and performance."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights can drive a positive business impact by guiding preventive maintenance scheduling and tool replacement strategies. By identifying wear levels where failure risk spikes, companies can plan interventions before breakdowns occur, reducing downtime and maintenance costs.\n",
        "However, if maintenance is done too frequently based on minor wear (over-interpretation), it could lead to negative growth through unnecessary operational halts and increased maintenance expenses. Therefore, insights must be applied judiciously, balancing cost and reliability."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the exploratory data analysis and visualizations, the following three hypothetical statements are defined to test relationships between machine failure and key numeric variables: torque, rotational speed, and tool wear."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average torque of failed machines is significantly different from non-failed machines.\n",
        "\n",
        "Null Hypothesis (Hâ‚€): There is no significant difference in mean torque between failed and non-failed machines.\n",
        "\n",
        "Alternative Hypothesis (Hâ‚): There is a significant difference in mean torque between failed and non-failed machines."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "t0 = train_path[train_path['machine_failure']==0]['torque_nm'].dropna()\n",
        "t1 = train_path[train_path['machine_failure']==1]['torque_nm'].dropna()\n",
        "res = stats.ttest_ind(t0, t1, equal_var=False)\n",
        "print('Torque t-test: stat,p', res.statistic, res.pvalue)\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent t-test (Welchâ€™s t-test)"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The torque variable is continuous and approximately normally distributed for both groups. Welchâ€™s t-test is chosen because it compares the means of two independent samples without assuming equal variances."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of rotational speed differs significantly between failed and non-failed machines.\n",
        "\n",
        "\n",
        "Null Hypothesis (Hâ‚€): The distribution of rotational speed is the same for failed and non-failed machines.\n",
        "\n",
        "Alternative Hypothesis (Hâ‚): The distribution of rotational speed is significantly different for failed and non-failed machines."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "a = train_path[train_path['machine_failure']==0]['rotational_speed_rpm'].dropna()\n",
        "b = train_path[train_path['machine_failure']==1]['rotational_speed_rpm'].dropna()\n",
        "res2 = stats.mannwhitneyu(a, b, alternative='two-sided')\n",
        "print('Rotational speed Mann-Whitney: stat,p', res2.statistic, res2.pvalue)"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mannâ€“Whitney U Test (Non-parametric test)"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rotational speed data is continuous but not normally distributed. Mannâ€“Whitney U test is a non-parametric alternative to the t-test that compares the median ranks between two groups when normality assumptions are not met."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mean tool wear time differs significantly between failed and non-failed machines.\n",
        "\n",
        "\n",
        "Null Hypothesis (Hâ‚€): There is no significant difference in mean tool wear between failed and non-failed machines.\n",
        "\n",
        "Alternative Hypothesis (Hâ‚): There is a significant difference in mean tool wear between failed and non-failed machines."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "a = train_path[train_path['machine_failure']==0]['tool_wear_min'].dropna()\n",
        "b = train_path[train_path['machine_failure']==1]['tool_wear_min'].dropna()\n",
        "res3 = stats.ttest_ind(a, b, equal_var=False)\n",
        "print('Tool wear t-test: stat,p', res3.statistic, res3.pvalue)\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent t-test (Welchâ€™s t-test)"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tool wear is a continuous variable and shows roughly normal distribution. The test is appropriate for comparing means between two independent groups with possibly unequal variances."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Tests\n",
        "\n",
        "| Variable         | Test Used      | Type           | Key Insight                                                          |\n",
        "| ---------------- | -------------- | -------------- | -------------------------------------------------------------------- |\n",
        "| Torque           | Welchâ€™s t-test | Parametric     | Torque differs significantly between failed and non-failed machines. |\n",
        "| Rotational Speed | Mannâ€“Whitney U | Non-parametric | Failure patterns vary with speed distribution.                       |\n",
        "| Tool Wear        | Welchâ€™s t-test | Parametric     | Tool wear time is a significant factor in predicting failures.       |\n"
      ],
      "metadata": {
        "id": "UXtNll5Yor4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "train_path.isna().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_proc = train_path.copy()\n",
        "\n",
        "# Encode categorical 'type' column if it exists\n",
        "if 'type' in df_proc.columns:\n",
        "    df_proc['type_enc'] = df_proc['type'].map({'L': 0, 'M': 1, 'H': 2})\n",
        "\n",
        "# Drop unnecessary columns\n",
        "drop_cols = [c for c in ['product_id', 'type'] if c in df_proc.columns]\n",
        "df_proc.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
        "\n",
        "# Ensure we drop interval-type columns (like tool_bin)\n",
        "interval_cols = [c for c in df_proc.columns if pd.api.types.is_interval_dtype(df_proc[c])]\n",
        "df_proc.drop(columns=interval_cols, inplace=True)\n",
        "\n",
        "target = 'machine_failure'\n",
        "features = [c for c in df_proc.columns if c not in [target, 'id']]\n",
        "print('Features:', features)\n",
        "\n",
        "X = df_proc[features].copy()\n",
        "y = df_proc[target].copy()\n",
        "\n",
        "# Select only numeric columns for imputation\n",
        "X = X.select_dtypes(include=[np.number])\n",
        "\n",
        "# Handle missing values (median imputation)\n",
        "imp = SimpleImputer(strategy='median')\n",
        "X_imp = pd.DataFrame(imp.fit_transform(X), columns=X.columns)"
      ],
      "metadata": {
        "id": "oNdHCgexr9Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle outliers by clipping to 1st and 99th percentile\n",
        "for c in X_imp.columns:\n",
        "    lo, hi = X_imp[c].quantile(0.01), X_imp[c].quantile(0.99)\n",
        "    X_imp[c] = X_imp[c].clip(lo, hi)"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X_imp), columns=X_imp.columns)"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Encode categorical variable 'type'\n",
        "if 'type' in train_path.columns:\n",
        "    le = LabelEncoder()\n",
        "    train_path['type_encoded'] = le.fit_transform(train_path['type'])\n",
        "    print(\"âœ… 'type' column encoded using Label Encoding.\")\n",
        "else:\n",
        "    print(\"âš ï¸ No categorical column found for encoding.\")\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Feature Manipulation ---\n",
        "# Make a working copy\n",
        "train = train_path.copy()\n",
        "\n",
        "# Drop 'id' if it exists\n",
        "if 'id' in train.columns:\n",
        "    train = train.drop(columns=['id'])\n",
        "    print(\"âœ… Dropped 'id' column as it is not relevant.\")\n",
        "\n",
        "# Select only numeric columns for correlation analysis\n",
        "numeric_cols = train.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = train[numeric_cols].corr()\n",
        "\n",
        "# Identify highly correlated pairs (above 0.9 threshold)\n",
        "high_corr = [\n",
        "    (i, j) for i in corr_matrix.columns for j in corr_matrix.columns\n",
        "    if i != j and abs(corr_matrix.loc[i, j]) > 0.9\n",
        "]\n",
        "print(\"Highly correlated pairs:\", high_corr)\n",
        "\n",
        "# Optionally, you can drop one of each correlated pair\n",
        "if high_corr:\n",
        "    to_drop = list({j for _, j in high_corr})  # keep unique columns to drop\n",
        "    train = train.drop(columns=to_drop)\n",
        "    print(f\"âœ… Dropped highly correlated columns: {to_drop}\")\n",
        "else:\n",
        "    print(\"âœ… No highly correlated columns found.\")\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "if 'torque_nm' in train.columns:  #Only if features are skewed\n",
        "    train['torque_log'] = np.log1p(train['torque_nm'])\n",
        "    print(\"âœ… Applied log transformation to 'torque_nm'.\")"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Identify numeric columns\n",
        "num_cols = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X_imp), columns=X_imp.columns)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the StandardScaler technique to scale the numerical features in the dataset. This method standardizes data by centering the mean at zero and scaling the variance to one, ensuring all features are on a similar scale. Standardization is important because many machine learning algorithms, such as logistic regression, XGBoost, and LightGBM, are sensitive to the magnitude of input variables. Without scaling, features with larger values can dominate model learning, leading to biased results. StandardScaler helps improve model performance, ensures faster convergence during training, and maintains consistency across features, making it an ideal choice for this dataset."
      ],
      "metadata": {
        "id": "CmCA_tevwRiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction wasnâ€™t needed as the dataset had few, meaningful, and non-redundant features, ensuring interpretability and strong predictive performance."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print('âœ… Train/Val shapes:', X_train.shape, X_val.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An 80:20 train-test split was used to balance training and evaluation data, ensuring sufficient learning and reliable validation. Stratified sampling preserved class distribution, improving fairness and generalization in model performance.\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cw = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "print('class weights:', {cls:wt for cls,wt in zip(np.unique(y_train), cw)})\n",
        "\n",
        "lr = LogisticRegression(class_weight='balanced', max_iter=2000); lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_val); y_proba_lr = lr.predict_proba(X_val)[:,1]\n",
        "def metrics(y_true, y_pred, y_proba):\n",
        "    return {'accuracy':accuracy_score(y_true,y_pred),\n",
        "            'precision':precision_score(y_true,y_pred, zero_division=0),\n",
        "            'recall':recall_score(y_true,y_pred),\n",
        "            'f1':f1_score(y_true,y_pred),\n",
        "            'roc_auc':roc_auc_score(y_true,y_proba)}\n",
        "print('LR metrics:', metrics(y_val, y_pred_lr, y_proba_lr))"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Logistic Regression model was used as a baseline to predict machine failures. Itâ€™s a simple, interpretable linear model suitable for binary classification. The modelâ€™s performance was evaluated using accuracy, precision, recall, F1-score, and ROC-AUC. It achieved balanced results, with recall being most important for detecting failures, providing a reliable foundation for comparison with advanced models."
      ],
      "metadata": {
        "id": "Zl9401zSzMQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_val); y_proba_rf = rf.predict_proba(X_val)[:,1]\n",
        "print('RF metrics:', metrics(y_val, y_pred_rf, y_proba_rf))\n",
        "\n",
        "# feature importances\n",
        "fi = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "display(fi.head(15))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save pipeline\n",
        "joblib.dump({'imputer': imp, 'scaler': scaler, 'model': rf}, \"C:\\\\Users\\\\ASUS\\\\Downloads\\\\TATA_stl.joblib\")\n",
        "print(\"Saved pipeline to C:\\\\Users\\\\ASUS\\\\Downloads\\\\TATA_stl.joblib\")"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully developed a predictive model for machine failure detection using sensor data from industrial equipment. Through detailed exploratory data analysis, hypothesis testing, and feature engineering, key factors influencing failuresâ€”such as torque, rotational speed, and tool wearâ€”were identified. Machine learning models, particularly Random Forest and Logistic Regression, were applied, achieving strong recall and balanced accuracy. This enables early fault detection and preventive maintenance, reducing downtime and operational costs. The model pipeline was optimized, validated, and saved for real-world deployment. Overall, this project demonstrates the power of data-driven solutions in enhancing industrial reliability and efficiency."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}